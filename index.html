<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>GenForce</title>
  <meta name="description" content="GenForce: Training Tactile Sensors to Learn Force Sensing from Each Other">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="icon" type="image/png" href="./images/logo.png">

  <!-- Fonts and CSS -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans:400,500,700|Noto+Sans:400,600|Castoro:400,700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <!-- JS -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>

  <style>
    :root {
      --maxw: 1100px;
      --accent: #0f62fe; 
    }
    html, body {
      font-family: "Noto Sans", system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif;
      color: #1f2937;
      scroll-behavior: smooth;
    }
    .container.is-max-desktop { max-width: var(--maxw); }
    .publication-title { line-height: 1.25; }
    .section { padding-top: 3rem; padding-bottom: 3rem; }
    .content p, .content li { line-height: 1.75; font-size: 1.05rem; }
    .subtitle-muted { color: #6b7280; }

    /* Authors and affiliations */
    .authors { font-size: 1.05rem; line-height: 1.6; margin-top: .75rem; }
    .authors a { color: #1f2937; text-decoration: none; }
    .authors a:hover { text-decoration: underline; }
    .authors sup { font-size: 0.7em; top: -0.6em; }
    .affiliations { margin-top: .5rem; color: #4b5563; font-size: .98rem; }
    .affiliations li { list-style: none; }
    .notes { color: #6b7280; font-size: .95rem; margin-top: .25rem; }

    /* Buttons row */
    .link-row .button { margin: .25rem .35rem; }
    .button.is-dark { background: #111827; }
    .button.is-dark:hover { background: #000; }

    /* Figures and captions */
    figure { margin: 1.25rem auto; }
    figure img { width: 100%; height: auto; border-radius: 6px; }
    figcaption { color: #6b7280; font-size: .95rem; margin-top: .4rem; }
    .box.section-box { padding: 2rem 1.5rem; }

    /* Hero */
    .hero .hero-body { padding-top: 3rem; padding-bottom: 2rem; }
    .kicker { letter-spacing: .08em; text-transform: uppercase; color: var(--accent); font-weight: 700; font-size: .85rem; }

    /* Video grid: two columns on >= 769px, one column on mobile */
    .video-grid {
      display: grid;
      grid-template-columns: 1fr;
      gap: 26px;
      margin-top: 18px;
    }
    @media (min-width: 769px) {
      .video-grid { grid-template-columns: 1fr 1fr; }
    }
    .video-item {
      background: #fff;
      border: 1px solid #e5e7eb;
      border-radius: 10px;
      padding: 12px;
      box-shadow: 0 1px 2px rgba(0,0,0,.03);
    }
    .ratio-16x9 {
      position: relative; width: 100%; padding-bottom: 56.25%; overflow: hidden; border-radius: 8px;
    }
    .ratio-16x9 iframe {
      position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: 0;
    }
    .video-title { font-weight: 600; margin: 10px 2px 0; }
  </style>
</head>

<body>

  <!-- Hero / Title -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <p class="kicker">GenForce</p>
            <h1 class="title is-2 publication-title">
              Training Tactile Sensors to <br> Learn Force Sensing from Each Other
            </h1>

            <style>
              .authors .lead { color: #e91e63; }     /* lead authors */
              .authors .co   { color: #333; }        /* co-authors */
            </style>

            <div class="authors">
              <p>
                <a class="lead" href="https://zhuochenn.github.io/">Zhuo Chen<sup>1*</sup></a>,
                <a class="co">Ni Ou<sup>1</sup></a>,
                <a class="co">Xuyang Zhang<sup>1</sup></a>,
                <a class="co">Zhiyuan Wu<sup>1</sup></a>,
                <a class="co">Yongqiang Zhao<sup>1</sup></a>,
                <a class="co">Yupeng Wang<sup>1</sup></a>,<br>
                <a class="co">Emmanouil Spyrakos Papastavridis<sup>1</sup></a>,
                <a class="co" href="https://lepora.com/">Nathan Lepora<sup>2</sup></a>,
                <a class="co" href="https://lorejam.blogspot.com/">Lorenzo Jamone<sup>3</sup></a>,  
                <a class="lead" href="https://jiankangdeng.github.io/">Jiankang Deng<sup>4*</sup></a>,
                <a class="lead" href="https://shanluo.github.io/">Shan Luo<sup>1*</sup></a>
              </p>

              <ul class="affiliations">
                <li><sup>1</sup> King’s College London, London, United Kingdom</li>
                <li><sup>2</sup> University of Bristol, Bristol, United Kingdom</li>
                <li><sup>3</sup> University College London, London, United Kingdom</li>
                <li><sup>4</sup> Imperial College London, London, United Kingdom</li>
                <li><sup>*</sup> Corresponding Authors</li>
              </ul>
             
              <!-- <p class="notes">* These authors contributed equally. ✉ Correspondence: your.email@kcl.ac.uk</p> -->
            </div>

            <!-- Action buttons -->
            <div class="link-row" style="margin-top: 1rem;">
              <a target="_blank" href="https://www.researchsquare.com/article/rs-6513579/v1" class="button is-small is-rounded is-dark">
                <span class="icon"><i class="fas fa-file"></i></span><span>Preprint</span>
              </a>
              <a target="_blank" href="" class="button is-small is-rounded is-dark">
                <span class="icon"><i class="fab fa-github"></i></span><span>Code(To release)</span>
              </a>
              <a target="_blank" href="" class="button is-small is-rounded is-dark">
                <span class="icon"><i class="fas fa-database"></i></span><span>Dataset (To release)</span>
              </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Abstract / Overview -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <p class="is-size-5">
              Humans achieve stable and dexterous object manipulation by coordinating grasp forces across multiple fingers and palms, facilitated by a unified tactile memory system in the somatosensory cortex. This system encodes and stores tactile experiences across skin regions, enabling the flexible reuse and transfer of touch information.
              Inspired by this biological capability, we present <strong style="color:#e91e63">GenForce, the first framework that enables transferable force sensing across tactile sensors in robotic hands</strong>. GenForce unifies tactile signals into <strong>shared marker representations</strong>, analogous to cortical sensory encoding, <strong>allowing force prediction models trained on one sensor to be transferred to others without the need for exhaustive force data collection</strong>.
              We demonstrate that GenForce generalizes across both <strong style="color:#e91e63">homogeneous sensors with varying configurations</strong> and <strong style="color:#e91e63">heterogeneous sensors with distinct sensing modalities and material properties</strong>. This transferable force sensing is also demonstrated with high performance in <strong>robot force control including daily object grasping, slip detection and avoidance</strong>. Our results highlight a <strong>scalable paradigm for robotic tactile learning</strong>, offering new pathways toward <strong>adaptable and tactile memory–driven manipulation</strong> in unstructured environments.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Background -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="box section-box">
        <h2 class="title is-3 has-text-centered">Background</h2>
        <figure>
          <img src="images/Background.png" alt="Background">
          <figcaption><strong style="color:#a00ee4">Robot grasping objects with tactile sensors and force control</strong> mimics human actions with sensory receptors. 
            These bio-inspired tactile sensors cannot transfer force data with each other due to <strong>differences in sensing principles, 
            structural designs and material properties</strong>. Current practice to train force prediction models uses <strong>repetitive and costly data collection process</strong> for force labels.</figcaption>
        </figure>
      </div>
    </div>
  </section>

  <!-- Human tactile memory -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="box section-box">
        <h2 class="title is-3 has-text-centered">Human Tactile Memory</h2>
        <figure>
          <img src="images/Tactile_memory.png" alt="Human tactile memory">
          <figcaption> In humans, the <strong style="color:#a00ee4">tactile memory system</strong> enables the <strong>storage and retrieval</strong> of experienced tactile information, 
             such as haptic stimuli, <strong>across skin regions on hands</strong>. Mechanoreceptors in the skin detect deformation, which is translated into a <strong>unified sensory encoding</strong>, 
             and transmitted to the somatosensory cortex via peripheral nerves for storage and processing. 
             This human ability to <strong>adapt, unify, and transfer tactile sensation</strong> offers valuable inspiration for developing <strong>transferable tactile sensing</strong> in robots. </figcaption>
        </figure>
      </div>
    </div>
  </section>

  <!-- Bioinspiration -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="box section-box">
        <h2 class="title is-3 has-text-centered">Bioinspiration</h2>
        <figure>
          <img src="images/Motivation.png" alt="Bioinspiration">
          <figcaption><strong style="color:#a00ee4">Overview of the GenForce model</strong>. Tactile sensors produce <strong>diverse tactile signals under 
            the same deformation</strong> due to differences in sensing principles, structural designs and material properties. 
            GenForce <strong>unifies tactile signals into marker representation</strong>, enables <strong>marker-to-marker translation</strong> across various sensors, 
            and achieves <strong>high-accuracy force prediction</strong> on uncalibrated sensors using data transferred from calibrated sensors.</figcaption>
        </figure>
      </div>
    </div>
  </section>

  <!-- Architecture -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="box section-box">
        <h2 class="title is-3 has-text-centered">Architecture</h2>
        <figure>
          <img src="images/Architecture.png" alt="Architecture of GenForce">
          <figcaption><strong style="color:#a00ee4">Marker-to-marker translation (M2M) model</strong>. The M2M model uses <strong>deformed images from calibrated sensors as input</strong> 
            and <strong>reference images from uncalibrated sensors as conditions</strong> to <strong>generate deformed images</strong> that mimic the deformation applied to uncalibrated sensors.  
            <strong>Spatiotemporal force prediction model</strong> takes sequential contact images as input and outputs three-axis forces, 
            enhancing prediction accuracy through a spatiotemporal module.</figcaption>
        </figure>
      </div>
    </div>
  </section>

<!-- Marker-to-marker Translation Performance -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="box section-box">
        <h2 class="title is-3 has-text-centered">Marker-to-marker Translation Performance</h2>
        <figure>
          <img src="images/m2m.png" alt="sim">
          <figcaption><strong style="color:#a00ee4">Marker-to-marker translation in simulated data</strong> shown with t-SNE.
            we <strong>first propose a simple simulation pipeline to acquire extensive deformed marker images</strong>. 
            densities—referring to <strong>GelSight, uSkin, TacTip and GelTip </strong>sensors. <strong>Eighteen 3D-printed indenters</strong> with diverse geometrical properties 
            (vertices, edges, and curvatures) are used for indentation33. Each marker pattern can serve as both source sensor and target sensor, 
            resulting in a total of <strong>132 sensor combinations</strong>.The generated images and target images are <strong>aligned closely in feature space and visually indistinguishable</strong>.
            <strong>See Supplementary Video 1</strong>.
           </figcaption>
        </figure>

        <figure>
          <img src="images/heter_m2m.png" alt="hetero_m2m" >
          <figcaption><strong style="color:#a00ee4">Marker-to-marker translation in heterogeneous sensors</strong> shown with images.
          We <strong>unify three distinct tactile signals into marker representation</strong>. For <strong>electronic sensor arrays</strong>, we develop a <strong>signal-to-marker pipeline</strong>
          that <strong>converts multichannel raw signals into marker displacement and diameter change</strong>. 
          Although <strong>some tactile arrays (e.g., capacitive or resistive)</strong> can only measure pressure in each taxel compared to magnetic sensor with 3-axis measurement, 
          <strong>our model still transfers from vision-based tactile sensors to these arrays</strong>. We verify this by using only the z component of uSkin, referred to as “uSkin (z‑axis)” in <strong>Supplementary Video 3</strong>.
          We showcase generated images and source images in above figure and the <strong>Supplementary Video 1</strong>.
          </figcaption>
        </figure>  

      </div>
    </div>
  </section>


  <!-- Force Prediction Performance -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="box section-box">
        <h2 class="title is-3 has-text-centered">Force Prediction Performance</h2>
        <figure>
          <img src="images/homo.png" alt="homo">
          <figcaption><strong style="color:#a00ee4">Homogeneous Translation Performance</strong> compared with ATI nano 17 F/T sensor before and after using GenForce.
          The <strong>source-only method exhibits large errors (Supplementary Video 2)</strong>. The maximum error in normal force exceeds 4.8 N. 
          Shear force errors are average above 0.28 N. In addition, most sensor combinations have negative R2 values and high variance 
          and demonstrate poor performance in real-time force prediction. <strong>After using the GenForce model, all force errors are significantly reduced, and R2 values improve across all combinations</strong>. 
          </figcaption>
        </figure>

        <figure>
          <img src="images/hetero.png" alt="hetero">
          <figcaption><strong style="color:#a00ee4">Heterogeneous Translation Performance</strong> compared with ATI nano 17 F/T sensor before and after using GenForce.
          The average of MAE over all six combinations decreases below 0.92 N for normal force, while Fx and Fy reduce below 0.22 N and 0.3 N, respectively.
           Notably, the uSkin_TacTip group shows a 93% improvement of in MAE (from 7.76 N to 0.52 N) and a 66% improvement of Fy (from 0.59 N to 0.2 N). 
           The force errors for all combinations are <strong>centered around zero within ranges of -4N to 0N in normal direction and -3N to 3N in shear direction, 
          demonstrating both the accuracy and reliability of our model.</figcaption>
        </figure>  

      </div>
    </div>
  </section>


<!-- Applications -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="box section-box">
        <h2 class="title is-3 has-text-centered">Applications</h2>
        <figure>
          <img src="images/Dynamic.png" alt="Dynamic">
          <figcaption><strong style="color:#a00ee4">Force prediction with dynamic contact events</strong> compared with ATI nano 17 F/T sensor
          We <strong>evaluated our model in real-time across six transfer groups</strong> under more dynamic conditions (see Supplementary Video 3). 
          Tactile sensors were mounted on an ATI Nano17 F/T sensor, and we applied forces using four daily objects with different shapes and materials, 
          including <strong>screwdriver, glue stick, plastic pizza, and a LEGO block</strong>  . 
          A human operator performed five common dynamic contact events, including <strong>press, rub, roll, push, and pull, 
          as well as continuous combinations of these on the sensor surface</strong>. All test groups exhibited <strong>fast, accurate responses comparable to a commercial F/T sensor</strong>.
          </figcaption>
        </figure>

        <figure>
          <img src="images/Grapsing.png" alt="Grapsing">
          <figcaption><strong style="color:#a00ee4">Daily objects grasping with transferable force sensing and control using GelSight (A-II) and uSkin (3 axis)</strong>. 
            <strong>We transfer the force prediction model to above two sensors by using a third flat-surface vision-based tactile sensor (GelSight, marker pattern A-II) tactile sensor with our GenForce model</strong> . 
            The task requires robot to grasp nine daily objects with different sizes, shapes and material without damage them. 
            Those objects include potato <strong>chip, grape, strawberry, orange, plum, wood block, glue stick, meat box and tea box, 
            which are unseen in the training dataset</strong>. During the grasping, the arm is controlled with a proportional controller to 
            grasp those objects with fixed normal forces ranging from 0.6N to 1.2N. Both sensors are shared with the same force controller. 
            As shown in above figure and Supplementary Video 4&5, both sensors can equip the robot arm an accurate force sensing so that the robot arm can <strong>successfully grasp all objects with target forces without damage</strong>. 
            Even for challenging objects such as <strong>chips and fresh fruits<strong>, the robot can achieve delicate grasping by using the transferred force prediction model combining with <strong>force control</strong>. </figcaption>
        </figure>

        <figure>
          <img src="images/Slip.png" alt="Slip">
          <figcaption><strong style="color:#a00ee4">Transferable force sensing in robot slip detection and avoidance</strong>. 
            A curved surface, vision based TacTip sensor (palm shape) is mounted on the left finger, 
            and a three-axis magnetic uSkin sensor is mounted on the right. The TacTip's force prediction model 
            is transferred from GelSight (D-I), and the uSkin model is subsequently transferred from the TacTip. 
            The task proceeds through <strong>several stages—moving down, proportional control grasping, lifting, 
            slip detection and avoidance at the top position, release, and return to home—with the force controller</strong> 
            active only during the grasp and slip detection phases. We evaluate <strong>four objects—banana, plum, meat box, 
            and glue stick</figcaption>. Beyond completing the grasp, external forces are applied by a human at the top position to induce slip. 
            The robot detects slip via changes in shear force and responds by narrowing the gripper width. As shown in above figure and<strong> Supplementary Videos 6-7</strong>, 
            <strong>the system completes all stages successfully</strong>, demonstrating the practical applications of our model in real robotic tasks.</figcaption>
        </figure>     

      </div>
    </div>
  </section>

  <!-- Demo Videos (Two-column responsive grid) -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">
        <span style="vertical-align: middle;">
          <img src="images/video_logo.png" alt="Video icon" width="40" style="vertical-align: middle; margin-right: 8px;">
        </span>
        Supplementary Videos
      </h2>

      <div class="video-grid">
        <!-- 1 -->
        <div class="video-item">
          <div class="ratio-16x9">
            <iframe src="https://www.youtube.com/embed/WH0H29L9_80?si=z8Al2-NOFQrqe9iV"
              title="Marker-to-marker translation" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
              referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </div>
          <div class="video-title" style="text-align:center;">Marker-to-marker translation</div>
        </div>

        <!-- 2 -->
        <div class="video-item">
          <div class="ratio-16x9">
            <iframe src="https://www.youtube.com/embed/VcYdDlSoOmQ?si=r_yl8_HCiNCYfoDA"
              title="Force Prediction Performance" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
              referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </div>
          <div class="video-title" style="text-align:center;">Force Prediction Performance</div>
        </div>

        <!-- 3 -->
        <div class="video-item">
          <div class="ratio-16x9">
            <iframe src="https://www.youtube.com/embed/d8XDvgV8BY8?si=gfQ74cKPjZZyqvA5"
              title="Dynamic Force Test" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
              referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </div>
          <div class="video-title" style="text-align:center;">Dynamic Force Test</div>
        </div>

        <!-- 4 -->
        <div class="video-item">
          <div class="ratio-16x9">
            <iframe src="https://www.youtube.com/embed/eEBpvs0Urks?si=kWQ3xHpEP0cK3tDV"
              title="Daily object grasping (1)" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
              referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </div>
          <div class="video-title" style="text-align:center;">Daily object grasping (1)</div>
        </div>

        <!-- 5 -->
        <div class="video-item">
          <div class="ratio-16x9">
            <iframe src="https://www.youtube.com/embed/fs-T0AL-C_I?si=Pbt53NgDW6g-nG_D"
              title="Daily object grasping (2)" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
              referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </div>
          <div class="video-title" style="text-align:center;">Daily object grasping (2)</div>
        </div>

        <!-- 6 -->
        <div class="video-item">
          <div class="ratio-16x9">
            <iframe src="https://www.youtube.com/embed/TeEXjSpj1PY?si=c5wIPC8gym5S_1LP"
              title="Slip Detection & Avoidance (1)" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
              referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </div>
          <div class="video-title" style="text-align:center;">Slip Detection &amp; Avoidance (1)</div>
        </div>

        <!-- 7 -->
        <div class="video-item">
          <div class="ratio-16x9">
            <iframe src="https://www.youtube.com/embed/d0Dqasav5zo?si=LhX8XY8lJNE9l6D1"
              title="Slip Detection & Avoidance (2)" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
              referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </div>
          <div class="video-title" style="text-align:center;">Slip Detection &amp; Avoidance (2)</div>
        </div>

      </div>
    </div>
  </section>

  <!-- Citation -->
  <section class="section" id="Citation">
    <div class="container is-max-desktop">
      <div class="box section-box">
        <h2 class="title is-3 has-text-centered">Citation</h2>
        <div class="content">
          <p>If you find our model helpful, feel free to cite it:</p>
<pre><code>@article{chen2025general,
  title={General Force Sensation for Tactile Robot},
  author={Chen, Zhuo and Ou, Ni and Zhang, Xuyang and Wu, Zhiyuan
          and Zhao, Yongqiang and Wang, Yupeng and Lepora, Nathan
          and Jamone, Lorenzo and Deng, Jiankang and Luo, Shan},
  journal={arXiv preprint arXiv:2503.01058},
  year={2025}
}</code></pre>
        </div>
      </div>
    </div>
  </section>

  <footer class="section" style="padding-top: 1.5rem; padding-bottom: 2rem;">
    <div class="container is-max-desktop has-text-centered">
      <p class="subtitle-muted">© 2025 GenForce Authors</p>
    </div>
  </footer>

</body>
</html>